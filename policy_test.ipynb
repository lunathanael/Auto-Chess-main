{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsQbIaJY8S482RXTiLp1sD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lunathanael/Auto-Chess-main/blob/master/policy_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "aEOVZHf_fRtr"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import typing\n",
        "from typing import Dict, List, Optional\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Add, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAXIMUM_FLOAT_VALUE = float('inf')\n",
        "\n",
        "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])"
      ],
      "metadata": {
        "id": "RIE-oRMbDOlS"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinMaxStats(object):\n",
        "  \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
        "\n",
        "  def __init__(self, known_bounds: Optional[KnownBounds]):\n",
        "    self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
        "    self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
        "\n",
        "  def update(self, value: float):\n",
        "    self.maximum = max(self.maximum, value)\n",
        "    self.minimum = min(self.minimum, value)\n",
        "\n",
        "  def normalize(self, value: float) -> float:\n",
        "    if self.maximum > self.minimum:\n",
        "      # We normalize only when we have set the maximum and minimum values.\n",
        "      return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "    return value"
      ],
      "metadata": {
        "id": "6ZCEDG5ADcEx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def residual_block(x, filters):\n",
        "    \"\"\"Create a residual block.\"\"\"\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(x)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = ReLU()(y)\n",
        "    y = Conv2D(filters, kernel_size=3, padding='same')(y)\n",
        "    y = BatchNormalization()(y)\n",
        "    y = Add()([y, x])\n",
        "    y = ReLU()(y)\n",
        "    return y\n",
        "\n",
        "def make_network():\n",
        "  # Input layer\n",
        "  input_layer = Input(shape=(8, 8, 73))\n",
        "\n",
        "  # Body\n",
        "  x = Conv2D(256, kernel_size=3, padding='same')(input_layer)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = ReLU()(x)\n",
        "  for _ in range(19):\n",
        "      x = residual_block(x, 256)\n",
        "\n",
        "  # Policy Head\n",
        "  policy_head = Conv2D(256, kernel_size=3, padding='same')(x)\n",
        "  policy_head = BatchNormalization()(policy_head)\n",
        "  policy_head = ReLU()(policy_head)\n",
        "  policy_head = Conv2D(73, kernel_size=1)(policy_head)\n",
        "\n",
        "  # Value Head\n",
        "  value_head = Conv2D(1, kernel_size=1)(x)\n",
        "  value_head = BatchNormalization()(value_head)\n",
        "  value_head = ReLU()(value_head)\n",
        "  value_head = Dense(256, activation='relu')(value_head)\n",
        "  value_head = Dense(1, activation='tanh')(value_head)\n",
        "\n",
        "  # Create the model\n",
        "  model = tf.keras.Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "  return model\n",
        "\n",
        "def ConstantPolicyHead(shape):\n",
        "    # Custom layer for policy head\n",
        "    class CPHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(1/73, shape=shape)\n",
        "\n",
        "    return CPHead()\n",
        "\n",
        "def ConstantValueHead():\n",
        "    # Custom layer for value head\n",
        "    class CVHead(tf.keras.layers.Layer):\n",
        "        def call(self, inputs):\n",
        "            return tf.constant(0.5, shape=(1,))\n",
        "\n",
        "    return CVHead()\n",
        "\n",
        "def make_uniform_network():\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=(8, 8, 73))\n",
        "\n",
        "    # Body\n",
        "    x = Conv2D(256, kernel_size=3, padding='same')(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x, 256)\n",
        "\n",
        "    # Modified Policy Head for uniform output\n",
        "    policy_head = ConstantPolicyHead((8, 8, 73))(x)\n",
        "\n",
        "    # Modified Value Head for constant output\n",
        "    value_head = ConstantValueHead()(x)\n",
        "\n",
        "    # Create the model with constant outputs\n",
        "    model = Model(inputs=input_layer, outputs=[policy_head, value_head])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ABgGpmx0fWGi"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_board_game_config(action_space_size: int, max_moves: int,\n",
        "                           dirichlet_alpha: float,\n",
        "                           lr_init: float) -> Config:\n",
        "\n",
        "  def visit_softmax_temperature(num_moves, training_steps):\n",
        "    if num_moves < 30:\n",
        "      return 1.0\n",
        "    else:\n",
        "      return 0.0  # Play according to the max.\n",
        "\n",
        "  return Config(\n",
        "      action_space_size=action_space_size,\n",
        "      max_moves=max_moves,\n",
        "      discount=1.0,\n",
        "      dirichlet_alpha=dirichlet_alpha,\n",
        "      num_simulations=800,\n",
        "      batch_size=2048,\n",
        "      td_steps=max_moves,  # Always use Monte Carlo return.\n",
        "      num_actors=3000,\n",
        "      lr_init=lr_init,\n",
        "      lr_decay_steps=400e3,\n",
        "      visit_softmax_temperature_fn=visit_softmax_temperature,\n",
        "      known_bounds=KnownBounds(-1, 1))"
      ],
      "metadata": {
        "id": "Q30EAjHEDq1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_chess_config() -> MuZeroConfig:\n",
        "  return make_board_game_config(\n",
        "      action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)"
      ],
      "metadata": {
        "id": "FAIovKBJDxnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlZ53-pxD0Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers"
      ],
      "metadata": {
        "id": "P3mUyAMlzxsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "\n",
        "\n",
        "  def __init__(self,\n",
        "               action_space_size: int,\n",
        "               max_moves: int,\n",
        "               discount: float,\n",
        "               dirichlet_alpha: float,\n",
        "               num_simulations: int,\n",
        "               batch_size: int,\n",
        "               td_steps: int,\n",
        "               num_actors: int,\n",
        "               lr_init: float,\n",
        "               lr_decay_steps: float,\n",
        "               visit_softmax_temperature_fn,\n",
        "               known_bounds: Optional[KnownBounds] = None):\n",
        "    ### Self-Play\n",
        "    self.action_space_size = action_space_size\n",
        "    self.num_actors = num_actors\n",
        "\n",
        "    self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
        "    self.max_moves = max_moves\n",
        "    self.num_simulations = num_simulations\n",
        "    self.discount = discount\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = dirichlet_alpha\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    # If we already have some information about which values occur in the\n",
        "    # environment, we can use them to initialize the rescaling.\n",
        "    # This is not strictly necessary, but establishes identical behaviour to\n",
        "    # AlphaZero in board games.\n",
        "    self.known_bounds = known_bounds\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(1000e3)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = batch_size\n",
        "    self.num_unroll_steps = 5\n",
        "    self.td_steps = td_steps\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "\n",
        "    # Exponential learning rate schedule\n",
        "    self.lr_init = lr_init\n",
        "    self.lr_decay_rate = 0.1\n",
        "    self.lr_decay_steps = lr_decay_steps\n",
        "\n",
        "  def new_game(self):\n",
        "    return Game(self.action_space_size, self.discount"
      ],
      "metadata": {
        "id": "t32QDQxky6Yn"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n"
      ],
      "metadata": {
        "id": "Lvmt70d1y8HR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess; 11259 for shogi, 362 for Go\n",
        "\n",
        "  def terminal(self): #TODO\n",
        "    # Game specific termination rules.\n",
        "    pass\n",
        "\n",
        "  def terminal_value(self, to_play): #TODO\n",
        "    # Game specific value.\n",
        "    pass\n",
        "\n",
        "  def legal_actions(self): #TODO\n",
        "    # Game specific calculation of legal actions.\n",
        "    return []\n",
        "\n",
        "  def clone(self):\n",
        "    return Game(list(self.history))\n",
        "\n",
        "  def apply(self, action):\n",
        "    self.history.append(action)\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.itervalues())\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in range(self.num_actions)\n",
        "    ])\n",
        "\n",
        "  def make_image(self, state_index: int): #TODO\n",
        "    # Game specific feature planes.\n",
        "    return []\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2"
      ],
      "metadata": {
        "id": "rSCKGOM9zjwa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: AlphaZeroConfig):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = numpy.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, numpy.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos]"
      ],
      "metadata": {
        "id": "En06LbK6zoRk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "\n",
        "  def __init__(self, uniform_model: bool=False):\n",
        "    if (uniform_model):\n",
        "      self.model = make_uniform_network()\n",
        "    else:\n",
        "      self.model= make_network()\n",
        "\n",
        "  def inference(self, image):\n",
        "      # Run the neural network model to get predictions\n",
        "      value, policy_logits = self.model.predict(np.array([image]))\n",
        "\n",
        "      value = value[0][0] # The value output is a scalar representing the predicted game outcome\n",
        "\n",
        "      # The policy_logits are the raw outputs of the network representing move probabilities\n",
        "      policy = tf.nn.softmax(policy_logits).numpy().tolist()[0]\n",
        "\n",
        "      return value, policy\n",
        "\n",
        "\n",
        "  def get_weights(self): #TODO\n",
        "    # Returns the weights of this network.\n",
        "    return self.model.get_weights()"
      ],
      "metadata": {
        "id": "HPr_d9UOzqBG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SharedStorage(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.iterkeys())]\n",
        "    else:\n",
        "      return Network(True)  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n"
      ],
      "metadata": {
        "id": "6UsTnTQKzreQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AlphaZero training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def alphazero(config: AlphaZeroConfig):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  for i in range(config.num_actors):\n",
        "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n"
      ],
      "metadata": {
        "id": "tV9QkBmLBKT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: AlphaZeroConfig, network: Network):\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "  return select_action(config, game, root), root\n",
        "\n",
        "\n",
        "\n",
        "def softmax_sample(visit_counts, temperature=10.0):\n",
        "    counts, actions = zip(*visit_counts)\n",
        "    # Apply softmax with temperature\n",
        "    counts = np.array(counts)\n",
        "    counts = counts / temperature  # Apply temperature scaling\n",
        "    softmax_probs = np.exp(counts) / sum(np.exp(counts))\n",
        "    # Sample an action based on the softmax probabilities\n",
        "    action = np.random.choice(actions, p=softmax_probs)\n",
        "    return softmax_probs, action\n",
        "\n",
        "\n",
        "def select_action(config: AlphaZeroConfig, game: Game, root: Node):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.iteritems()]\n",
        "  if len(game.history) < config.num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: AlphaZeroConfig, node: Node):\n",
        "  _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                         for action, child in node.children.iteritems())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: AlphaZeroConfig, parent: Node, child: Node):\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.itervalues())\n",
        "  for action, p in policy.iteritems():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: AlphaZeroConfig, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = numpy.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "\n",
        "def train_network(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  optimizer = tf.keras.optimizers.SGD(config.learning_rate_schedule,\n",
        "                                         config.momentum)\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0:\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "def update_weights(optimizer: tf.keras.optimizers, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, (target_value, target_policy) in batch:\n",
        "    value, policy_logits = network.inference(image)\n",
        "    loss += (\n",
        "        tf.losses.mean_squared_error(value, target_value) +\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=policy_logits, labels=target_policy))\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ou5wkaIzpvvk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def fen_to_board_array(fen):\n",
        "    # Split the FEN string to get the relevant parts\n",
        "    parts = fen.split(' ')\n",
        "    board_fen, player, castling, _, halfmove, fullmove = parts[:6]\n",
        "\n",
        "    # Initialize an empty board with additional planes for L constant-valued inputs\n",
        "    board = np.zeros((12 + 5, 8, 8), dtype=float)  # 12 for pieces, 5 for additional planes\n",
        "\n",
        "    # Define piece order and mapping to layers\n",
        "    piece_map = {'p': 0, 'n': 1, 'b': 2, 'r': 3, 'q': 4, 'k': 5,\n",
        "                 'P': 6, 'N': 7, 'B': 8, 'R': 9, 'Q': 10, 'K': 11}\n",
        "\n",
        "    # Fill the board with pieces\n",
        "    row = 0\n",
        "    col = 0\n",
        "    for char in board_fen:\n",
        "        if char.isdigit():\n",
        "            col += int(char)\n",
        "        elif char == '/':\n",
        "            row += 1\n",
        "            col = 0\n",
        "        else:\n",
        "            board[piece_map[char], row, col] = 1\n",
        "            col += 1\n",
        "\n",
        "    # Add extra layers for player color, move number, etc.\n",
        "    # Player color (1 for black, 0 for white)\n",
        "    board[12, :, :] = 1 if player == 'b' else 0\n",
        "\n",
        "    # Castling rights encoded in four binary planes\n",
        "    board[13, :, :] = 1 if 'K' in castling else 0  # White kingside\n",
        "    board[14, :, :] = 1 if 'Q' in castling else 0  # White queenside\n",
        "    board[15, :, :] = 1 if 'k' in castling else 0  # Black kingside\n",
        "    board[16, :, :] = 1 if 'q' in castling else 0  # Black queenside\n",
        "\n",
        "    # Move number (as a real value)\n",
        "    board[17, :, :] = float(fullmove)\n",
        "\n",
        "    return board\n",
        "\n",
        "# Example usage\n",
        "fen = \"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\"\n",
        "board_array = fen_to_board_array(fen)\n"
      ],
      "metadata": {
        "id": "6Kb78XOxjZek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AlphaZeroConfig()\n",
        "network_1 = alphazero(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "tsgLx3x98qnB",
        "outputId": "d3befe5b-dba2-4dc7-e0d7-c6dd4132db5e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_7\" is incompatible with the layer: expected shape=(None, 8, 8, 73), found shape=(None, 0)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-c98dd61cb637>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaZeroConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphazero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36malphazero\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_selfplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36mlaunch_job\u001b[0;34m(f, *args)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlaunch_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m   \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36mrun_selfplay\u001b[0;34m(config, storage, replay_buffer)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(config, network)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mgame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_moves\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_search_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36mrun_mcts\u001b[0;34m(config, game, network)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_mcts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAlphaZeroConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0madd_exploration_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-576c44a51c94>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(node, game, network)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# We use the neural network to obtain a value and policy prediction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m   \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m   \u001b[0;31m# Expand the node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-10b7aaade213>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# Run the neural network model to get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# The value output is a scalar representing the predicted game outcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_7\" is incompatible with the layer: expected shape=(None, 8, 8, 73), found shape=(None, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjvDlKC-83UJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}